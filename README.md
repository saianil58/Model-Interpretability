# Model-Interpretability

Machine learning is in the center of the latest progress in technology and is an essential tool for accurate predictions nowadays. However, most of the time we neither can clearly identify nor explain the logic behind these predictions because the model is just too complex. In those cases our machine learning model is called a ’Black Box’.

![Image](https://d33wubrfki0l68.cloudfront.net/5331cb13d71df10783ce7b69c7bc9f703db5bf3d/2ecd6/img/posts/lime/intro.png)

So how do we know if we can trust this model? How should we be able to trust it, when we don’t even know how it actually makes it’s predictions?

These are important questions which occur when the challenges of Model explainability are presented, especially if it is used for decision making. Users need to be confident that the model will perform well. Gaining trust in predictions through increasing transparency of a black box model.

There are multiple python libraries for this task.
1. eli5
2. LIME
3. SHAP

This repo deals with such libraries and derives explanations of the balckbox models from pipelines. This is essential in explaning the model working to business.

Repo link (https://nbviewer.jupyter.org/github/saianil58/Model-Interpretability/blob/master/Introduction%20to%20Model%20Interpretability.ipynb)

Documentations of these are:

[ELI5](https://eli5.readthedocs.io/en/latest/)

[SHAP](https://github.com/slundberg/shap)

[LIME](https://github.com/marcotcr/lime)
